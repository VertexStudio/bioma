use bioma_mcp::{
    schema::{CallToolResult, TextContent},
    server::{Context, RequestContext},
    tools::{ToolDef, ToolError},
};
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use std::sync::Arc;

#[derive(Serialize, Deserialize, JsonSchema)]
pub struct AskArgs {
    #[schemars(description = "Query text to ask")]
    query: String,

    #[schemars(description = "Filter results by source name")]
    source_name: Option<String>,

    #[schemars(description = "Language model to use for answering")]
    model: Option<String>,

    #[schemars(description = "Include retrieved context in response")]
    include_context: Option<bool>,

    #[schemars(description = "Maximum number of context chunks to retrieve")]
    max_context_chunks: Option<usize>,
}

#[derive(Clone)]
pub struct Ask {
    context: Context,
}

impl Ask {
    pub fn new(context: Context) -> Self {
        Self { context }
    }
}

impl ToolDef for Ask {
    const NAME: &'static str = "ask";
    const DESCRIPTION: &'static str = "Ask a question and get an answer based on retrieved context (RAG)";
    type Args = AskArgs;

    async fn call(&self, args: Self::Args, _request_context: RequestContext) -> Result<CallToolResult, ToolError> {
        // In a real implementation, this would:
        // 1. Retrieve context based on the query
        // 2. Formulate a prompt with the retrieved context
        // 3. Send the prompt to the LLM
        // 4. Return the response

        let model = args.model.unwrap_or_else(|| "default-model".to_string());
        let source_filter = args.source_name.clone().unwrap_or_else(|| "all sources".to_string());
        let include_context = args.include_context.unwrap_or(false);
        let max_chunks = args.max_context_chunks.unwrap_or(3);

        // Mock retrieval
        let mock_contexts = vec![
            format!("Context 1 for query: '{}' from {}", args.query, source_filter),
            format!("Context 2 for query: '{}' from {}", args.query, source_filter),
            format!("Context 3 for query: '{}' from {}", args.query, source_filter),
        ];

        let contexts = mock_contexts.into_iter().take(max_chunks).collect::<Vec<_>>();

        // Mock LLM response
        let answer = format!(
            "Based on the retrieved information, the answer to '{}' is: This is a mock answer generated by model {}.",
            args.query, model
        );

        let response = if include_context {
            let context_text = contexts.join("\n\n");
            format!("{}\n\nSources:\n{}", answer, context_text)
        } else {
            answer
        };

        Ok(CallToolResult {
            content: vec![
                serde_json::to_value(TextContent { type_: "text".to_string(), text: response, annotations: None })
                    .map_err(ToolError::ResultSerialize)?,
            ],
            is_error: Some(false),
            meta: None,
        })
    }
}
